{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tree-based Methods"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn import tree\n",
    "from sklearn.model_selection import train_test_split, cross_val_score\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "from sklearn.metrics import mean_squared_error\n",
    "%matplotlib inline\n",
    "plt.style.use('seaborn-white')\n",
    "RANDOM_STATE = 42"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Random Forest Trees"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('https://raw.githubusercontent.com/sukhjitsehra/datasets/master/CP322/Hitters.csv').dropna()\n",
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = df.drop(['Salary', 'League', 'Division', 'NewLeague'], axis= 1)\n",
    "y = np.log(df.Salary)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "### Question 1: \n",
    "\n",
    "Complete the following class to fit, predict and score a decision tree. \n",
    "\n",
    "<!--\n",
    "BEGIN QUESTION\n",
    "name: q1\n",
    "points: 5\n",
    "-->"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DecisionTreeModel:\n",
    "    def __init__(self, X_train, X_test, y_train, y_test, max_depth=None, max_leaf_nodes=None, min_samples_split=2, min_samples_leaf=1):\n",
    "        self.X_train = X_train\n",
    "        self.y_train = y_train\n",
    "        self.X_test = X_test\n",
    "        self.y_test = y_test\n",
    "        self.tree = DecisionTreeRegressor(max_depth=max_depth, max_leaf_nodes= max_leaf_nodes, min_samples_split=min_samples_split, min_samples_leaf=min_samples_leaf, random_state=RANDOM_STATE)\n",
    "        \n",
    "    def fit(self):\n",
    "        ...\n",
    "        return\n",
    "    \n",
    "    def predict(self):\n",
    "        self.y_pred = ...\n",
    "        return\n",
    "    \n",
    "    def calculate_metrics_regression(self):\n",
    "        y_pred = self.predict()\n",
    "        mse = ...\n",
    "        return mse\n",
    "        \n",
    "    def plot(self):\n",
    "        plt.figure(figsize=(15,10))\n",
    "        tree.plot_tree(self.tree, filled=True)\n",
    "        plt.show()\n",
    "    \n",
    "    def plot_importance(self):\n",
    "        Importance = pd.DataFrame({'Importance':self.tree.feature_importances_*100}, index=self.X_train.columns)\n",
    "        Importance.sort_values('Importance', axis=0, ascending=True).plot(kind='barh', color='r', )\n",
    "        plt.xlabel('Variable Importance')\n",
    "        plt.gca().legend_ = None\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "outputs": [],
   "source": [
    "grader.check(\"q1\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train test split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.10, random_state=RANDOM_STATE)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "Question 2: Use the above class to define a model using some values (`using hit and trial method`) of hyper-parameters so that the mse value is lower than .24.\n",
    "\n",
    "\n",
    "Note: In decision tree algorithms, hyperparameters are used to control the model complexity and prevent overfitting. Here are the explanations of the four hyperparameters you have asked for and generally, it's recommended to try different hyperparameter values and evaluate the performance of the resulting models using cross-validation:\n",
    "\n",
    "1. max_depth: This hyperparameter controls the maximum depth of the decision tree. The depth of a decision tree is the length of the longest path from the root node to a leaf node. Setting a high value for max_depth can lead to overfitting, while setting it too low may cause underfitting. Usually, it is recommended to set this hyperparameter to a value between 3 and 10, depending on the complexity of the problem and the size of the dataset. If the dataset is small, a smaller value can be used, while if the dataset is large and complex, a larger value may be necessary.\n",
    "\n",
    "2. max_leaf_nodes: This hyperparameter controls the maximum number of leaf nodes that can be present in the decision tree. This can be an alternative to max_depth to control the complexity of the tree. If both max_depth and max_leaf_nodes are set, the one that results in a smaller decision tree will be used. A value between 5 and 50 is often used as a starting point for this hyperparameter. However, the optimal value depends on the size of the dataset and the complexity of the problem.\n",
    "\n",
    "3. min_samples_split: This hyperparameter sets the minimum number of samples required to split an internal node. If the number of samples is less than min_samples_split, the node is not split, and it becomes a leaf node. This hyperparameter helps prevent overfitting and also affects the shape of the decision tree. This hyperparameter can be set to a value between 2% to 10% of the total number of samples in the dataset. A typical starting value for this hyperparameter is 2 or 3.\n",
    "\n",
    "4. min_samples_leaf: This hyperparameter sets the minimum number of samples required to be at a leaf node. If the number of samples is less than min_samples_leaf, the tree builder will try to split the node until there are at least min_samples_leaf samples in the leaf. This hyperparameter also helps prevent overfitting and affects the shape of the decision tree. This hyperparameter can be set to a smaller value than min_samples_split. A typical starting value is 1 or 2, but it can be increased depending on the size of the dataset and the complexity of the problem.\n",
    "\n",
    "<!--\n",
    "BEGIN QUESTION\n",
    "name: q2\n",
    "points: 5\n",
    "-->"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "fit_decision = ...\n",
    "fit_decision.fit()\n",
    "fit_decision.predict()\n",
    "mse = fit_decision.calculate_metrics_regression()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "outputs": [],
   "source": [
    "grader.check(\"q2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "fit_decision.plot_importance()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
